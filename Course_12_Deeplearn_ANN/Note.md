# 深度学习与神经网络

## 一、神经网络

### 1.1 定义

生物神经网络--**感知器**:

1. 外部刺激通过神经末梢, 转换为电信号, 传导神经元
2. 无数神经元构成神经中枢

![neurons_struct](https://askabiologist.asu.edu/sites/default/files/resources/articles/neuron_anatomy.jpg)

神经元工作原理:

1. 处理的输入信息\<阈值: 神经元处于抑制状态
2. 处理的输入信息>阈值: **神经元处于激活状态**

人工神经元:

1. 接其他n个神经元的信号, 进行一个**加权求和**
2. 把预激活的加权结构传递给激活函数(激活与否和神经元有效无效无关)

神经元和人工神经元对比:

![ANN](https://miro.medium.com/max/1400/1*hkYlTODpjJgo32DoCOWN5w.png)

神经网络: **多个神经元组成的就是神经网络**

1. 前馈神经网络: DNN，DBN，CNN
2. 反馈神经网络: CHNN，RNN

目标: 找到一个**未知函数的近似值**, 原理受到大脑神经网络启发

### 1.2 神经元

神经元是组成神经网络的最基本单位，其基本结构为:

![single_a_n](https://upload.wikimedia.org/wikipedia/commons/6/60/ArtificialNeuronModel_english.png)

其中 $x_1, x_2, x_3, ..., x_n$ 为输入，$w_{1_j}, w_{2_j}, w_{3_j}, ..., w_{n_j}$ 为每一个输入对应的权重，权重初始时**随机的**，用来表达输入的重要性。$\theta_j$ 为偏置，$\psi$ 为激活函数，使得结果非线性，最后得到输出，整体公式可以表示为:

$$
\begin{equation}
Output_j = \psi(\sum_{i=1}^nx_iw_{ij}+ \theta_j)
\end{equation}
$$

> 神经元分类的本质: **将特征空间切半，认为两半分属两个类**，其中切半的函数可以用n维泰勒展开，即加权求和的过程

神经元缺点: 只能一刀切，需要引入多层神经元，即多层神经网络

### 1.3 神经网络实质

一维: $y=wx+b$ 能够一分为二

高维: $h = a_1x_1 + a_2x_2 + ... + a_nx_n + 0 = 0$ 一分为二

参考高维公式，感知做的就是多个维度的加权求和

实质: **特征空间一切对半，认为两半分别属于两个类**

缺点: 只能一刀切

解决方法: 多层神经网络

1. 神经网络是一种运算模型，由大量节点(**神经元**)和之间相互联接构成
2. 每个节点间的联接代表通过该连接信号的加权值

每个节点的连线得到的权值都不一样，每个节点都是一个神经元，每个层做不同处理

### 1.4 多层神经网络

多层神经网络(多层感知机)基本结构:

![ann_struct](https://miro.medium.com/max/1200/1*3fA77_mLNiJTSgZFhYnU0Q.png)

此时多个神经元构成感知机，多层感知机构成神经网络(**相互全连接**)

1. 神经网络由多个神经元组成, **前一个神经元的结果作为后一个的输入**
2. 神经网络一般三层: 第一层输入, 最后一层输出, 中间隐藏层(可以多个)

### 1.5 激活函数

关于激活函数:

1. 激活函数是神经网络核心单元
2. 激活函数作用**在神经网络中引入非线性的学习和处理能力**
3. 常用激活函数(满足 1.非线性、2.可微、3.单调)

几个常用的激活函数:

![activation_functions](https://1394217531-files.gitbook.io/~/files/v0/b/gitbook-legacy-files/o/assets%2F-LvBP1svpACTB1R1x_U4%2F-LvNWUoWieQqaGmU_gl9%2F-LvO3qs2RImYjpBE8vln%2Factivation-functions3.jpg?alt=media&token=f96a3007-5888-43c3-a256-2dafadd5df7c)

1. sigmoid: $y=logsig(x)=\frac{1}{1+e^{-x}}$
2. tanh: $y=tansig(x)=\frac{e^x-e^{-x}}{e^x+e^{-x}}$
3. ReLU: $f(x)=max(0,x)$

引入激活函数的原因:

![why_activation](https://picx.zhimg.com/80/v2-9e3af4e88d56b4e034bd07688dd50bbe_1440w.webp?source=1940ef5c)

#### (1) sigmoid

形式:

![sigmoid](https://miro.medium.com/max/640/1*a2e-ozEcNCm_s0trFzf-DA.webp)

公式:

$$
\begin{equation}
y=logsig(x)=\frac{1}{1+e^{-x}}
\end{equation}
$$

缺点:

1. 梯度饱和(两边的梯度值都为0)
2. 结果均值不为0, 对后层神经元输入非0均值信号(0均值减小迭代次数)
3. 计算exp比较耗时

#### (2) tanh

形式:

![tanh](https://miro.medium.com/max/640/1*IcQ-P-cKAyt6N4vkwgCrdg.webp)

公式:

$$
\begin{equation}
y=tansig(x)=\frac{e^x-e^{-x}}{e^x+e^{-x}}
\end{equation}
$$

缺点:

1. 仍然存在梯度饱和

优点:

1. 解决原点对称问题(0均值)
2. 比sigmoid快

#### (3) Relu线性整流层

形式:

![relu](https://miro.medium.com/max/640/1*njuH4XVXf-l9pR_RorUOrA.webp)

公式:
$$
\begin{equation}
f(x)=max(0,x)
\end{equation}
$$

RELU起源: 从生物的脑神经元激活出发，模拟激活模型

缺点:

1. 梯度弥散没完全解决，负半轴相当于神经元死亡(0代表神经元死亡)

优点:

1. 解决部分梯度弥撒(饱和)的问题
2. 收敛速度更快

> 选取激活函数主要依据在模型中的性能

#### (4) 神经元稀疏

1. ReLU分段线性函数，所有负值均为0，正值不变，**单层抑制**
2. 模型增加N层，理论上**ReLU神经元的激活率降低2的N次方倍**

稀疏的作用: 大脑工作具备稀疏性，即**生物神经元同时被激活数量有限**

> 不用激活函数，中间层都可以合并(前一层的输出作为后一层的输入)

## 二、隐藏层的感性认识

### 2.1 张量 tensor

定义: 算法运行的一种数据结构，用于存储数据。

特征: 维度，零维张量就是常量，一维张量就是**数组**，二维张量就是二维数组

如下就是一个 $(3, 2, 2)$ 的张量，有**3**个元素(二维张量)，每个元素内有**2**个元素，张量内部包含**2**个常量

```python
x = np.array([
    [
        [1,2],
        [3,4]
    ],
    [
        [5,6],
        [7,8]
    ],
    [
        [9,10],
        [11,12]
    ]
])

print(x.ndim)
```

输出:

```cmd
3
```

可以用 $(n,c,h,w)$ 来表示维度，张量的选择主要取决于**使用的框架**，以及不同的硬件

一般来说，$n$: 数量，$c$: 通道数，$h$: 高度，$w$: 宽度

> 1).$(n, c, h, w)$: **cpu/gpu** </br> 2).$(n, h, w, c)$: **npu/gpu** </br> 3).$(n, c, t)$

数据存储于tensor(张量)中, 权重也可以存在其中

### 2.2 设计神经网络

> 确定输入，输出，隐藏层的层数

1. 使用神经网络训练数据前，必须确定**神经网络层数**，以及每层单元个数 (输入层和输出层是可以确定的)
2. 特征向量在传入输入层先**标准化到0-1(归一化)**(目的**加速**学习过程)
3. 离散型变量可以被编码成每一个输入单元和一个特征值可能赋的值
    1. 如果特征A可以用 $(a_0, a_1, a_2)$ 三个输入单元表示
    2. $A = a_0$，就有 $(1, 0, 0)$
    3. $A = a_1$，就有 $(0, 1, 0)$
    4. 对于特征更多，且包含的输入单元更多，就不太适用，可以用别的方法
4. 神经网络可以用来做分类也可以做回归
    1. 对于分类，如果是两类，可以用一个输出单元表示(0或1)；如果超过两类，**每多一个就增加一个输出单元**(如三类可以用这个表示: 100/010)，可以类比到特征输入
    2. 没有明确规则确定隐藏层的设计(**层数在实验前不确定**)，需要根据实验测试和误差以及精准度来实验和改进

> 上述的表示方法: **onehot表述方法**，更加直观

### 2.3 对隐藏层的感性认识

对于人脸而言，存在一堆特征需要判断(**特征提取**):

1. 顶部头发?
2. 中部眼睛?
3. 底部嘴?

隐藏层的作用就是回答这些问题，即进行**特征的判断**，对于每一个问题可以再进行拆解(**下一层的隐藏层**):

1. 有眼球?
2. 有睫毛?
3. 有虹膜?

对于子网络可以一层层分解，直到判断可以在**单一的神经元**上回答

判断: 特征提取求和通过一刀切来看

黑盒: 取其中一个节点没办法知道他在回答什么问题(没有办法进行解释)

## 三、深度学习

1. 传统的神经网络发展到多层
2. 具有**多个隐藏层**的神经网络称为深度神经网络，基于深度神经网络的学习
3. 如果要细分和优化，深度学习对传统的多层神经网络(全连接ANN)进行优化

深度学习就是多层的人工神经网络，最重要的功能就是**表征学习**(从底层-中层-高层), 进行分类

![resentation](<https://www.baeldung.com/wp-content/uploads/sites/4/2022/03/cnn_layers-1024x257.png>)

> 关于深度学习的表征学习的体现，比较关键的一点就是深度学习只是将**特征-特征**之间的转换模式以**层-层**之间实现，其中**高维特征向量**以**层**的形式表示。所以越深的網路代表著經過多次的函數處理跟萃取，所萃取的資訊的抽象程度越高，抽象程度越高，就越接近人類所想像的。

对于深度学习而言，越深的网络代表经过多次的函数处跟萃取，所萃取的咨询的抽象程度越高，越接近人类的想象，具体可以参考[深度學習其實是一種 Representation learning](https://ithelp.ithome.com.tw/articles/10202169)

人工智能，机器学习，深度学习之间的关系可以表示为:

![relations](https://www.tibco.com/sites/tibco/files/media_entity/2020-09/deep-learning-diagram.svg)
