# 推理和训练

## 一、推理和训练

### 1.1 监督/非监督

**监督学习**: 有标签, 关键方法分类和回归, 比如逻辑回归和BP神经网络

**无监督学习**: 无标签, 关键是规则学习和聚合, k-means

### 1.2 概念

**训练(Training)**: 一个初始神经网络通过不断优化自身参数来变准确

**推理(Inference)**: 对现场数据(live data)进行识别, 准确率高, 则性能好

训练的和推理的对应(对于未知类无法判断)

### 1.3 优化和泛化

深度学习的根本问题是优化和泛化的对立

**优化(optimization)**: 调节模型以在训练数据上得到最佳性能(学习)

**泛化(generalization)**: 指训练好的模型在前所未见的数据上的性能好坏

### 1.4 数据集的分类

1. 训练集: 实际训练算法的数据集, 用来计算梯度，确定每次迭代中网络权值的更新

2. 验证集: 用于跟踪其学习效果的数据集, 是一个指示器, 用于表明训练数据点之间所形成的网络函数发生(中间结果的检查)

3. 测试集: 用于产生最终结果的数据集

测试集要有效反应网络泛化能力:
    1. 测试集不能以任何形式用于训练网络, 即使是用于同一组备选网络中挑选网络。测试集只能在所有的训练和模型选择完成后使用
    2. 测试集必须代表网络使用中涉及的所有情形

为了简便有时候验证集会省略, 主要用途是阶段测试

训练集中不一定要包含负样本, 一个输出可以表示二分类

### 1.5 交叉验证

切分A, B, C:
    1. A测试, BC训练
    2. B测试, AC训练
    3. C测试, AB训练

通过计算三者准确度的平均值

### 1.6 bp神经网络

**BP网路(Back-Propagtion Network)**: 误差逆向传播算法训练的多层前馈网络

误差反向传播的过程就是训练过程之一

正向传播(输入到输出): 权值一开始随机, 输出得到值与期望过大, 计算误差反向传播

反向传播(输出到输入): 按照梯度下降, 不断调整神经元的连接权值和阈值

### 1.7 神经网络训练

利用神经网络去解决图像分割, 边界探测等问题 $y = f(x)$

**f不是一个简单的线性函数**,

神经网络 -> 训练(正向, 反向)

神经网络 -> 推理: 用现场数据进行正向传播(不进入反向, 过程和训练一致, 不需要判断误差)

训练: 迭代是机器自己调整参数, 而调参是人为的调整参数

不同硬件计算会不同:
    1. cpu -> 卷积
    2. gpu -> 二叉树

硬件会影响计算结果

## 二、训练的相关概念

### 2.1 神经网络训练

代(Epoch): 全部数据进行一次完整训练, 称为“一代训练”

批大小(Batch Size): 使用训练集的一笑部分样本进行一次反向传播, 这一小批样本为一批

迭代(Iteration): 使用Batch数据进行一次参数更新, 称为“一次训练”(一次迭代), 每一次迭代得到的结果会作为下一次的初始值。一个迭代 = 一个正向通过 + 一个反向通过

$Number of Batches = Training Set Size / Batch Size$

如: 训练集500样本, batchsize = 10, 训练完样本: iteration=50, epoch=1(为了训练效果更好, 也可以增加)

不同epoch之间使用同一个训练集, 模型值不同

### 2.2 训练流程

1. 提取特征向量作为输入

2. 定义神经网络结构, 隐藏层数, 激活函数

3. 通过训练利用反向传播算法不算优化权重值, 使之达到合理水平

4. 使用训练好的神经网络来预测未知数据(推理), 训练好的网络: 权重达到最优的情况

### 2.3 神经网络训练过程

1. 选择样本集合的一个样本(A_i, B_i), 前为数据, 后为标签

2. 送入网络, 计算网络的实际输出Y, 此时网络中权重应该都是随机量

3. 计算误差 $D=B_i - Y$ (预测值和实际值的差)

4. 根据误差D来调整权重矩阵W

5. 对样本重复上述过程, 直到整个样本集, 误差不超过规定范围

### 2.4 更具体

1. 参数随机初始化

2. 前向传播计算每个样本输出节点激活函数值

3. 计算损失函数

4. 反向传播计算偏导数

[可视化的训练过程](https://playground.tensorflow.org/)

## 三、训练的步骤和涉及的问题

### 3.1 参数的随机初始化

神经网络没有隐藏层, 参数可以初始化全为0

初始化方式很多: 1. 区间随机, 2. XAvier初始化, 3. relu正态

### 3.2 标准化

进行分类器和模型的建立训练时, 去除单位限制, 转换为无量纲的纯数值, 便于不同单位量级的指标进行比较加权

最典型: 归一化处理 $y = (x-min) / (max-min)$

z-score标准化(零均值归一化)
    1. 处理后数据均值均为0, 标准差为1(正态分布)
    2. μ为均值, σ为方差
    3. $y=(x-μ) / σ$

### 3.3 损失函数

描述模型预测值与真实值差距, **均值平方差(MSE)** 和 **交叉熵**

1.均值平方差(MSE), 均方误差:

$MSE = SUM^n_{i=1} \frac{1}{n} (f(x_i)- y_i)^2$

2.交叉熵(cross entropy), 用于预测输入的样本在哪一类的概率, 值越小, 预测结果越准

$C = - \frac{1}{n} SUM$

损失函数的选取取决于输入标签数据的类型:

### 3.4 梯度下降法

梯度的方向函数值增大的方向, 梯度的模表示函数值增大的速率(基于损失函数, 计算梯度的反方向)

不断将参数值向梯度反方向更新, 得到函数的最小值(全局最小值或局部最小值)

(找MSE的梯度反方向, 更新w, 得到一个 f`(x) 来减少MSE)

一般利用梯度更新会乘以一个小于1的**学习速率(learning rate)**, 这是因为往往梯度的模比较大, 直接更新参数会使函数值不算波动, 很难收敛(一般取0.8)

$θ_{t+1} = θ_t - α_t * f(θ_t)$

### 3.5 学习率

学习率是一个重要超参数, 控制基于损失梯度调整神经网络权值的速度

学习率越小, 沿着损失梯度下降速度越慢

新权值 = 当前权值 - 梯度 * 学习率

### 3.6 泛化能力

欠拟合: 模型不能很好表现数据结构

拟合: 测试误差与训练误差差距较小

过拟合: 模型过分拟合训练样本, 对于测试样本预测准确率不高

#### 过拟合

数据有噪声, 会把噪声也拟合

造成模型比较复杂, 使得泛化能力差

出现的原因:

    1. 样本选取
    2. 噪声过大
    3. 假设模型无法合理存在
    4. 参数太多导致模型复杂度过高
    5. 神经网络: 分类决策不唯一, 迭代次数足够多但又缺少代表性的特征

解决方法:

    1. 减少特征: 删除与目标不相关特征
    2. Early stopping
    3. 更多的训练样本
    4. 重新清洗数据
    5. Dropout

#### Early Stopping

每一个Epoch结束时, 计算准确率, 准确率不再提升就停止

记录准确率, 连续10个Epoch没达到最佳, 认为不再提高, 停止

#### Dropout

dropout通过修改神经网络本身结构来实现

1. 训练开始时, 随机删除隐藏神经元

2. 根据BP对参数更新, 下一次迭代, **继续随机删除**(降低隐藏层的复杂度)

修改ANN中隐藏的神经元个数来减少过拟合

能实现的原因:

1. 每次训练网络不一样

2. 隐藏节点以概率随机出现, 不能保证2个隐节点每次都同时出现

通过交叉验证, 隐藏节点drop out率等于0.5效果最好

drop out可以视作一种添加噪声的方法

缺点:

训练时间是没有drop out的两道三倍

### 3.7 计算实例

见笔记
