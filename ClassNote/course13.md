# 深度学习与神经网络

## 一、神经网络

### 1.1 定义

生物神经网络--**感知器**:

1. 外部刺激通过神经末梢, 转换为电信号, 传导神经元
2. 无数神经元构成神经中枢

处理的输入信息\<阈值: 神经元处于抑制状态

处理的输入信息>阈值: **神经元处于激活状态**

人工神经网络:

1. 接其他n个神经元的信号, 进行一个**加权求和**

2. 把预激活的加权结构传递给激活函数

多个神经元组成的就是神经网络

目标: 找到一个**未知函数的近似值**, 原理受到大脑神经网络启发

### 1.2 神经元

神经元是组成神经网络的最基本单位

weight: 描述输入数据对后续工作的重要性

感知结果: $sum = W*X + b$

激活函数f: 使得结果**非线性**

### 1.3 神经网络实质

一维: $y=wx+b$ 能够一分为二

高维: $h = a_1x_1 + a_2x_2 + ... + a_nx_n + 0 = 0$ 一分为二

参考高维公式, 感知做的就是多个维度的加权求和

实质: **特征空间一切对半, 认为两半分别属于两个类**

缺点: 只能一刀切

解决方法: 多层神经网络

1. 神经网络是一种运算模型, 由大量节点(**神经元**)和之间相互联接构成
2. 每个节点间的联接代表通过该连接信号的加权值

每个节点的连线得到的权值都不一样, 每个节点都是一个神经元, 每个层做不同处理

### 1.4 多层神经网络

1. 神经网络由多个神经元组成, 前一个神经元的结果作为后一个的输入
2. 神经网络一般三层: 第一层输入, 最后一层输出, 中间隐藏层(可以多个)

### 1.5 前馈神经网罗

### 1.6 激活函数

1. 激活函数是神经网络核心单元
2. 激活函数作用**在神经网络中引入非线性的学习和处理能力**
3. 常用激活函数(满足 1.非线性、2.可微、3.单调)

常见: sigmodi, tanh, ReLu

输出: $a = w_1x_1 + w_2x_2 + b$

激活函数: $y = σ(a)$

#### sigmoid

-:

1. 梯度饱和
2. 结果均值不为0, 对厚层神经元输入非0均值信号(0均值减小迭代次数)

#### tanh

-:

1. 梯度饱和

+:

1. 解决原点对称问题
2. 比sigmoid快

#### Relu 线性整流层

$f(x) = max(0,x)$

从生物的脑神经元激活出发

-:

1. 梯度弥散没完全解决, 负半轴相当于神经元死亡

+:

1. 解决部分梯度弥撒(饱和)的问题
2. 收敛速度更快

### 1.7 神经元稀疏

1. ReLU分段线性函数, 所有负值均为0, 正值不变, **单层抑制**
2. 模型增加N层, 理论上**ReLU神经元的激活率降低2的N次方倍**

稀疏的作用: 大脑工作具备稀疏性

不用激活函数 ———— 中间层都可以合并

## 二、隐藏层

### 2.1 张量 tensor

是算法运行的一种数据结构

张量的一大特征就是维度, 0维张量=常量, 数组一维张量, 一个二维数组就是二维张量

```python
x = np.array([
    [
        [1,2],
        [3,4]
    ],
    [
        [5,6],
        [7,8]
    ],
    [
        [9,10],
        [11,12]
])

print(x.ndim)
```

输出:

```shell
3
```

表示方式: (3, 2, 2) 3个元素, 每个元素二维张量, 张量内部包含2个常量

可以用(n,c,h,w)来表示维度, 张量的选择主要取决于使用的框架, 以及不同的硬件

n: 数量, c: 通道数, h: 高度, w: 宽度

> (n, c, h, w): cpu/gpu </br> (n, h, w, c): npu/gpu </br> (n, c, t)

数据存储于tensor(张量)中, 权重也可以存在其中

### 2.2 设计神经网络

1. 使用神经网络训练数据前, 必须确定神经网络层数, 以及每层单元个数 (输入层和输出层是可以确定的)

2. 特征向量在传入输入层先标准化到0-1(加速学习过程)

3. 离散型变量可以被编码成每一个输入单元和一个特征值可能赋的值
    1. 如 特征A 可以用 (a0, a1, a2)三个输入单元表示
    2. A= a0 -> (1, 0, 0)
    3. A= a1 -> (0, 1, 0)

4. 神经网络可以用来做分类也可以做回归
    1. 对于分类, 如果是两类, 可以用一个输出单元表示(0, 1), 多余两类, 每多一个多一个输出单元(1 0 0), (0 1 0) 可以类比到特征输入
    2. 没有明确规则确定隐藏层的设计, 需要根据实验测试和误差以及精准度来实验和改进

> 上述的表示方法: one hot 表述方法, 非常简单明了

### 2.3 对隐藏层的感性认识

一堆特征需要判断:
    1. 顶部头发?
    2. 中部眼睛?
    3. 底部嘴?

隐藏层的作用就是回答这些问题, 即进行特征的判断

对于每一个问题可以再进行拆解:
    1. 有眼球?
    2. 有睫毛?
    3. 有虹膜?

对于子网络可以一层层分解, 直到判断可以在单一的神经元上回答

判断: 特征提取求和通过一刀切来看

黑盒: 取其中一个节点没办法知道他在回答什么问题(没有办法进行解释)

## 三、深度学习

1. 传统的神经网络发展到多层

2. 具有多个隐藏层的神经网络称为深度神经网络, 基于深度神经网络的学习

3. 如果要细分和优化, 神经网络对传统的多层神经网络进行优化

深度学习, 就是多层的人工神经网络

最重要的功能就是表征学习(从底层-中层-高层), 进行分类

### 3.1 选择的原因

随着数据量上升, 表现增加
