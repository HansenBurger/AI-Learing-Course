# 机器学习的数学基础

## 一、向量

### 1.1 定义

向量: 大小和方向 a^→
标量: 只有方向
单位向量:
    i: x, 1
    j: y, 1

### 1.2 线性变换

加法不变性: 两个向量和的变换等于各自变换的和 T(v+w) = T(v) + T(w)
乘法不变性: T(cv) = cT(v)

用来做机器学习和视觉的线性变换(矩阵乘法)

## 二、矩阵

### 2.1 定义

充满数字的表格

### 2.2 矩阵加减法

A+B=B+A (对应位置相加)
A与B的行列一致(同型矩阵)
**程序检查**: 矩阵加法前保证大小一致(防呆检查)

### 2.3 矩阵乘法

A_n = B_m
方程组到空间的简单表示

### 2.4 单位向量

n*n矩阵，对角线微1，其他为0
AI = A
IA = A

### 2.5 逆矩阵

A A^-1 = A^-1 A = I
(可逆矩阵一定是方阵)

### 2.6 奇异矩阵

没有逆矩阵的矩阵(|A|的倒数没有定义)
线性相关矩阵无逆矩阵
**程序检查**: det(a)

### 2.7 矩阵转置

(A^T)^T = A

(AB)^T = B^T A^T

### 2.8 对称矩阵

转置后等于原矩阵
**一个矩阵的转置和这个矩阵的乘积就是一个对称矩阵**

A^T = A (对称矩阵)

### 2.9 欧式变换

旋转+平移(坐标系的变换)

a' = Ra + t

R(旋转), t(平移)

### 2.10 齐次坐标

(1, 2) 齐次坐标 (1, 2, 1) 用N+1维来代表N维坐标

(X,Y) -> (x,y,w) X=x/w, Y=y/w

(1, 2, 0) 相当于变换到无穷大，代表无穷远的点

## 三、导数和偏导数

### 3.1 导数(微分)

导数代表函数斜率，描述函数**变化快慢的量**，同时曲线的极大值也可以用导数来判断，极大值点导数为0，斜率为0

### 3.2 偏导数

多元函数下，对每个变量求导，其他变量视作常量

物理意义: 产看一个变量在其他不变的情况下对函数的影响程度

## 四、梯度

### 3.1 定义

梯度本意为一个**向量**

表示函数在改点沿着这个方向(梯度方向)变化最快(变化率最大，梯度的模)

多元函数各自求偏导，偏导写成向量，就是梯度

### 3.2 梯度下降

定义: 寻找函数极小值的方法

做法: 在已知**参数当前值**的情况下，按当前点对应的**梯度向量的反方向**，并按照实现定好的**步长**大小，对参数进行调整

反方向(负梯度)

存在的问题:
    1. 参数调整缓慢
    2. 收敛于局部最小值

## 五、概率学基础

### 5.1 事件与关系的运算

交、并、补

交换律，结合律，分配律

### 5.2 概率的基本概念

#### (1) 概念

1. 任何事件A: P(A) >= 0
2. 必然事件B: P(B) = 1

#### (2) 性质

2.P(A-B) = P(A) - P(AB)

### 5.3 独立性

P(AB) = P(A)P(B)

A与B相互独立

### 5.4 离散

离散就是不连续

### 5.5 期望、方差、标准差

1.期望 均值: E(x) = x1p1 + x2p2 + x3p3

2.方差 刻画x与期望的偏离程度D(x)

    D(x) = E(x^2) - [E(X)]^2

3.标准差(均方差) 方差的算是平方根，反应数据的离散成度

平均值相同标准差未必相同

### 5.6 正态分布(高斯分布)

正态分布 N(μ, σ^2) μ: 中心位置(中心线), σ: 胖瘦

标准正态分布: μ=0, σ=1

## 六、熵

### 6.1 信息量

信息量: 信息多少的度量

I = -log(p)

### 6.2 熵

不确定性越大，信息量越大，熵越大
不确定性越少，信息量越少，熵越小

Ent(A) = -SUM(k=1,n)pklog2pk 信息熵

所有事件概率相等时，熵值最大

深度学习与熵的概念有关，损失函数计算:熵计算(联合熵，条件熵，**交叉熵**)

## 七、KL散度

KL散度: 衡量两个概率的分布匹配程度，**分布差异越大，KL散度越大**
