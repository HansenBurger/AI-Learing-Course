# 图像聚类方法

## 一、分类与聚类

### 1.1 分类

定义: 分类是从特定的数据中挖掘模式, 作出判断的过程(**有监督**)

过程:

1. 训练数据集存在一个**类标记号**, 判断是正向数据集(积极作用)，还是负向数据集(抑制作用)
2. 通过对数据集学习训练, 构建训练模型
3. 通过模型对预测数据预测, 评测性能  

### 1.2 聚类

定义: 将数据集中某些方面相似的数据成员放在一起(**无监督**)

特点:

1. 没有表述数据类别的分类和分组信息, **即这些数据是没有标签的**, 通过被归为无监督学习
2. 聚类的目的也是把数据分类, 完全根据算法判断数据间的相似性, 人为的进行解释

样本间属性:

1. 有序属性(0.1, 0.2): 数据大小反应样本程度
2. 无序属性(男, 女): 没有数据大小反应属性

> 对于聚类算法本身不受样本有序和无序影响

聚类算法类别:

1. 原型聚类(k均值聚类算法)
2. 层次聚类
3. 密度聚类

## 二、K均值聚类(K-means)

目标将数据点划分k个簇，优点运算快，缺点需要指定k值

### 2.1 原理

流程:

1. 确定K值(人为设定), K值得设定不影响流程
2. 从数据集中随机选择K个数据点作为质心(Centroid)或数据中心
3. 分别计算每个点到每个质心之间的距离, 并将每个点划分到最近质心的小组
4. 每个质心聚集了一些点后，重新算出新得质心(对于每个簇, 计算均值得到新得K个质心)
5. 迭代执行3->4, 直到迭代终止条件满足为止(聚类结果不再发生变化)

### 2.2 k-means 样例

对于有X和Y两个特征的数据集P，通过K-means进行聚类，假定 $K=2$

|  Point  |  X  |  Y  |
|  ----  |  ----  |  ----  |
|  **P1**  |  0  |  0  |
|  **P2**  |  1  |  2  |
|  P3  |  3  |  1  |
|  P4  |  8  |  8  |
|  P5  |  9  |  10  |
|  P6  |  10  |  7  |

首先随机两个质心，并令 $\mu_1$=$P1$, $\mu_2$=$P2$，计算P1-P6的点到他们的欧式距离，可以得到:

|  Point  |  $\mu_1(0, 0)$  |  $\mu_2(1, 2)$  |
|  ----  |  ----  |  ----  |
|  P1  |  **0.00**  |  2.23  |
|  P2  |  2.23  |  **0.00**  |
|  P3  |  3.16  |  **2.24**  |
|  P4  |  11.3  |  **9.22**  |
|  P5  |  13.5  |  **11.3**  |
|  P6  |  12.2  |  **10.3**  |

与质心相近的分为一组，可以得到:

1. 组A: $P1$
2. 组B: $P2$、$P3$、$P4$、$P5$、$P6$

在每一组中依据特征的均值计算出一个新的质心:

$$
\begin{aligned}
X'=\frac{\sum^n_{i=1}X_i}{n} \\
Y'=\frac{\sum^n_{i=1}Y_i}{n}
\end{aligned}
$$

其中 $n$ 为组中样本的数量，组1中因为只有一个元素，所以质心不改变($\mu_1'=\mu_1$)，而 $\mu_2$ 变为 $\mu_2'=(6.2, 5.6)$，再次计算所有样本点到质心的欧式距离，可以得到:

|  Point  |  $\mu_1'(0,0)$  |  $\mu_2'(6.2,5.6)$  |
|  ----  |  ----  |  ----  |
|  P1  |  **0.00**  |  5.62  |
|  P2  |  **2.24**  |  6.32  |
|  P3  |  **3.16**  |  5.60  |
|  P4  |  11.30  |  **3.00**  |
|  P5  |  13.50  |  **5.22**  |
|  P6  |  12.20  |  **4.05**  |

此时依据到质心距离分组为:

1. 组A: $P1$、$P2$、$P3$
2. 组B: $P4$、$P5$、$P6$

依据之前的均值方法，再次选出新的质心 $\mu_1''=(1.33, 1)$，$\mu_2''=(9, 8.33)$，重复上述步骤，得到:

|  Point  |  $\mu_1''(1.33, 1)$  |  $\mu_2''(9, 8.33)$  |
|  ----  |  ----  |  ----  |
|  P1  |  **1.4**  |  12.0  |
|  P2  |  **0.6**  |  10.0  |
|  P3  |  **1.4**  |  9.5  |
|  P4  |  47.0  |  **1.1**  |
|  P5  |  70.0  |  **1.7**  |
|  P6  |  56.0  |  **1.7**  |

此时分组结果为:

1. 组A: $P1$、$P2$、$P3$
2. 组B: $P4$、$P5$、$P6$

分组结果不再变换，结果收敛，聚类结束。

> 其中聚合停止方式有三种: </br>1. 分组结果不改变 </br>2. 质心不再改变 </br>3. 质心误差在容忍范围内

**注意:** 确定质心用的永远是已经存在的点

### 2.3 图像处理

通过K-means可以实现图像分割、图像聚类、图像识别

可以实现在**不改变分辨率得情况下**量化压缩图像颜色, 实现乳香颜色层级分割

聚类结果不好可能与K值得设置有关

图像的k-means示例图:

![image_k-means](https://docs.opencv.org/3.4/oc_color_quantization.jpg)

### 2.4 优缺点

优点:

1. 简单快速
2. 对于处理大数据集, 算法保持高效率
3. 当结果簇密集, 效果较好

缺点:

1. 必须事先给出k(人为设定簇数)
2. 对于噪声和孤立点数据敏感(会被单独聚成一类)

## 三、层次聚类

定义: 层次聚类是一种很直观得算法, 一层一层得进行聚类

**层次法**: 凝聚层次聚类算法(自下而上), 分裂层次聚类算法(自上而下)

层次聚类示意图(样本间的层级关系):

![hierarchical_clustering](https://pic3.zhimg.com/80/v2-3aed2646f89280472646264b8a740242_1440w.webp)

类与类间距离计算方法: 最短距离法，最长距离法，中间距离法，类平均法

### 3.1 凝聚层次聚类

采用最小距离的凝聚层次聚类:

1. 每个对象看作一类, 计算两两之间最小距离
2. 将距离最小的两个类合并成一个新类
3. 重复算类与所有类之间的距离
4. 重复2和3, 直到所有类最终合并成一类

特点:

1. 没有一个全局的目标函数, 没有局部极小问题和选择初始点问题
2. 合并操作往往是最终的, 一旦和并两个簇之后不会撤销
3. 计算存储代价昂贵

### 3.2 层次聚类优缺点

优点:

1. 距离和规则的相似度容易定义，限制少
2. 不需要预先制定聚类数
3. 可以发现类的层次关系
4. 可以聚类成其它形状

缺点:

1. 计算复杂度太高
2. 奇异值也能产生很大影响
3. 算法很可能产生链状

### 3.3 层次聚类和分类流程

以凝聚型层次聚类为例，对A、B、C、D、E、F、G这七个样本进行聚类，样本对应值如下:

![samples](https://pic1.zhimg.com/80/v2-62d996310081c6c3e7609418cdcc5010_1440w.webp)

首先将每个样本设置为一类，并计算两两之间的欧式距离，公式如下(对于多维样本依然适用):

$$
\begin{equation}
D = \sqrt{\sum^n_{i=1}(X_i-y_i)^2}
\end{equation}
$$

可以得到一个基于两两欧式距离的矩阵:

![matrix_0](https://pic1.zhimg.com/80/v2-c4e5f8cbb4af63d05474c81e502bd38c_1440w.webp)

其中值最小的记为同一组，即组 $(B,C)$。对于计算其他样本到组 $(B,C)$ 的距离，可以分别计算组内每个样本到其他样本的距离，并求均值，公式如下:

$$
\begin{equation}
D = \frac{\sqrt{(B-A)^2}+\sqrt{(C-A)^2}}{2}
\end{equation}
$$

重复上述的步骤，得到新的欧式距离矩阵:

![matrix_1](https://pic2.zhimg.com/80/v2-29814e749487ea134e4aab7f24aab845_1440w.webp)

依据此时的最小距离构建出新的组 $(D,E)$，此时有复数个组内样本数大于1的组出现，对于他们间距离的计算，有三种方法:

1. SingleLinkage: 组合数据点中距离最近的两个数据点
2. CompleteLinkage: 组合数据点中距离最远的两个数据点
3. AverageLinkage: 组合数据点和其他所有数据点的平均距离(计算量大，但更合理)

对于组 $(D,E)$ 和 组 $(B,C)$ 间的距离就可以参考如下公式:

$$
\begin{equation}
D = \frac{\sqrt{(D-B)^2}+\sqrt{(D-C)^2}+\sqrt{(E-B)^2}+\sqrt{(E-C)^2}}{4}
\end{equation}
$$

重复上述运算，直到最后聚合成一类为止，整个计算过程可以概括为以下的层次聚类树:

![clustering_tree](https://pic1.zhimg.com/80/v2-46caf6729fb0348165abb9cef4be5168_1440w.webp)

想分两类, 从上往下竖线进行切割, 竖线下连接分为一类。

>当出现有距离相同的情况，哪个类中包含的样本少(先出现)就优先聚合哪个

### 3.4 k-means与层次聚类

1. 对于服从高斯分布用K-means好(k值一般是由实验得到的)
2. 对有类别之间存在层结构的数据(对所有大学专业进行分类), 用层次聚类比较好

> 一般来说K-means的效率都会比层次聚类高, 占用内存小

## 四、密度聚类(DBSCAN)

### 4.1 定义和流程

参数:

1. 邻域 $\epsilon$ (eps)
2. 形成高密度区域所需的最少点数(minPts)

![DBSCAM](https://www.mdpi.com/sensors/sensors-21-05715/article_deploy/html/images/sensors-21-05715-g003.png)

首先选取任意一个需要遍历的点，在他的基础上形成 半径为 $\epsilon$ 的领域，并进行判断，满足条件的邻域内的点会聚成一类，聚类过的点不会再参与聚类。

### 4.2 优缺点

优点:

1. 对噪声不敏感
2. 能发现任意形状的聚类

缺点:

1. 都是由参数决定的
2. 用固定参数识别聚类，但当聚类的稀疏程度不同时，相同的判定标准可能会破坏聚类的自然结构，即较稀的聚类会被划分为多个类或密度较大且离得较近的类会被合并成一个聚类。

## 五、谱聚类

扩展
