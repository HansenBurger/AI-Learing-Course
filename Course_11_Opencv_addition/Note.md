# OPENCV算法解析

## 一、OPENCV

定义:

1. opencv是开源的**计算机视觉库**
2. opencv源码用C和C++编写，提供了python，JAVA的接口
3. 应用广泛，包含计算机视觉的领域都能用到
4. 查opencv官方手册(因为包含的函数很多)，不需要全部记住，掌握查字典

**opencv图片读取为BGR**, 而不是RGB

特性:

1. 除读取顺序外，其他库均为RGB
2. 除PIL读img类，其他库读取均为numpy矩阵
3. 处理性能最好的为opencv

读取图片速度最快:

```python
img = cv2.imread("%image_loc%")
```

常见算法:

1. 读取imread，显示imshow，存储write
2. 彩色转灰度图
3. 图像几何变换。平移，缩放，旋转，插值(最邻近、双线性)
4. 对比增强。线性变换，伽马变换，直方图均衡化
5. 边缘检测。sobel，canny
6. 图像的二维滤波。cvFilter2D

## 二、最小二乘法

### 2.1 线性回归

对于方程 $y=2\times x$，为线性回归，其中 $2$ 为**回归系数**。

线性回归表示这些离散点**总体上最逼近**哪条直线

![linear_regression](https://pic4.zhimg.com/80/v2-33bf8ee7da9c687e8d8f7cb9eda20097_1440w.webp)

### 2.2 数学原理

1. 通过**最小化|误差的平方和**, 寻找数据的最佳函数匹配(找直线)
2. 最小二乘法可以简便地求得未知数据, 并使**误差的平方和最小**
3. 残差: $r_i = h(x_i) - y_i$ 其中 $h(x_i)$ 回归的估计值
4. 三种**范数**
    1. 无穷范数: 残差绝对值的最大值 $max_{1≤i≤m}|r_i|$
    2. 1-范数: 绝对残差和, 所有数据点的残差距离之和 $\sum^m_{i=1}|r_i|$
    3. 2-范数: 残差平方和: $\sum^m_{i=1}r^2_i$
5. 拟合程度, 拟合函数h(x)与待求解函数y之间的相似性，残差越小，相似性越高

> 使用2-范数的原因, 方便进行微分运算

### 2.2 最小二乘法(线性方程)

根据最小误差中2-范数的定义，就可以给出最小二乘法的公式:

$$
\begin{equation}
min\{E(k,b)\} = min\{\sum^N_{n=1}(y_n-(k\times x_n + b))^2\}
\end{equation}
$$

未知量 $k$ 和 $b$ 可以分别通过求偏导并令偏导数为0得到，具体计算如下:

首先计算偏导 $k$、$b$ 的偏导数可以得到:

$$
\begin{equation}
\frac{\partial{E(k,b)}}{\partial{k}} = 2 \sum^N_{n=1}(y_n-kx_n-b)(-x_n) = 0
\end{equation}
$$

$$
\begin{equation}
\frac{\partial{E(k,b)}}{\partial{b}} = -2 \sum^N_{n=1}(y_n-kx_n-b) = 0
\end{equation}
$$

对Eq.3进行化简可以得到参数 $k$ 和 $b$ 之间的关系:

$$
\begin{equation}
b = \frac{1}{N}\sum^N_{n=1}(y_n-kx_n) = \frac{1}{N}\sum^N_{n=1}y_n - \frac{k}{N}\sum^N_{n=1}x_n
\end{equation}
$$

又由均值公式可以得到:

$$
\begin{equation}
b = \bar{y} - k\bar{x}
\end{equation}
$$

带回到Eq.2就可以得到:

$$
\begin{equation}
\sum^N_{n=1}y_nx_n - \sum^N_{n=1}x_n\bar{y}  = k\sum^N_{n=1}{x_n}^2 - k\sum^N_{n=1}x_n\bar{x}
\end{equation}
$$

变换等式，k就等于:

$$
\begin{equation}
k = \frac{\sum^N_{n=1}y_nx_n - \bar{y}\times\sum^N_{n=1}x_n}{\sum^N_{n=1}{x_n}^2 - \bar{x} \times \sum^N_{n=1}x_n}
\end{equation}
$$

$$
\begin{equation}
k = \frac{\sum^N_{n=1}y_nx_n - \frac{1}{N}\sum^N_{n=1}y_n\times\sum^N_{n=1}x_n}{\sum^N_{n=1}{x_n}^2 - \frac{1}{N}\sum^N_{n=1}x_n \times \sum^N_{n=1}x_n}
\end{equation}
$$

最终参数 $k$ 和 $b$ 分别计算为:

$$
\begin{equation}
k = \frac{N\sum^N_{n=1}x_n\times y_n-(\sum^N_{n=1}x_n)(\sum^N_{n=1}y_n)}{N\sum^N_{n=1}(x_n)^2-(\sum^N_{n=1}x_n)^2}
\end{equation}
$$

$$
\begin{equation}
b = \frac{\sum^N_{n=1}y_n}{N}-k\times \frac{\sum^N_{n=1}x_n}{N}
\end{equation}
$$

利用最小二乘法拟合得到线性结果就可以用 $y=kx+b$ 表示。

### 2.3 最小二乘法(矩阵)

矩阵计算的最小二乘法，主要适用于，多项式拟合的场景，即:

$$
\begin{equation}
h_\theta(x) = \theta_0 + \theta_1 x + \theta_2 x^2 + ... + \theta_n x^n = X\Theta
\end{equation}
$$

计算一组参数，使得残差平方和最小，即:

$$
\begin{equation}
min\sum^n_{i=1}(h_\theta(x_i)-y_i)^2 = (X\Theta-Y)^T(X\Theta-Y)
\end{equation}
$$

对 $\theta$ 计算偏导可以得到:

$$
\begin{equation}
\theta = (X^TX)^{-1}X^TY
\end{equation}
$$

具体的计算可以参考:

![ls](https://pic2.zhimg.com/80/v2-f58072d39abc1f961dbef6881a8040cd_720w.webp)

### 2.4 优缺点

优点:

1. 求出的直线上并不经过所有点, 但和周围点的距离最短。
2. 简洁高效，不需要像梯度下降法那样迭代

缺点:

1. 对于误差相对敏感
2. 矩阵运算中需要计算逆矩阵，可能不存在，也可能会有较大计算量
3. 拟合函数非线性，不能使用最小二乘

### 2.5 参考链接

1. [最小二乘法（least sqaure method）](https://zhuanlan.zhihu.com/p/38128785)
2. [多项式最小二乘法拟合的python代码实现](https://zhuanlan.zhihu.com/p/262254688)

## 三、RANSAC

随机采样一致性 (random sample consensus)

### 3.1 基础介绍

定义:

1. RANSAC**是一种思想**, 一个求解已知模型的参数框架。**不限定某一特定问题, 可以是计算机是视觉问题，也可以是统计数学甚至经济学领域的模型参数估计问题**
2. RANSAC是一种迭代的方法，非确定性方法，可以通过迭代增加结果合理的概率。
3. **内群**数据可以通过几组模型参数来叙述数据分布, **离群**数据不适合模型化的数据。**RANSAC给定一组(通常很小)内群, 存在一个程序, 可以估算最佳解释或者适用于这一数据模型的参数。**

> 通俗解释，传递已知模型(数据分布)，求解描述模型的参数

比较优势:

1. 相比于最小二乘法**减少噪声点的干扰**(生产实践中的数据往往存在一定的偏差)
2. 模型确定, 最大迭代次数允许, RANSAC总能找到最优解(80%的误差时), 相对费时
3. 对于数据量大的，最小二乘法运算量大，计算速度慢

### 3.3 算法步骤

输入:

1. 一组观测数据(包含较大噪声和无效点)
2. 一个用于解释观测数据的参数化模型(**模型已知**), $y=ax+b$
3. 一些可信参数

> 最小二乘也是已知模型: $y = kx + b$

步骤:

1. 在数据中随机找几个点设定为内群
2. 计算适合内群的模型 $y=ax+b$ -> 中的a和b (可以是最小二乘法，只要得出参数即可，**方法任意**)
3. 把刚才没选到的点带入模型，计算是否为内群 (带入算出误差，看是否在**阈值内来判断**是否是内群)
4. 计算内群数量
5. 重复以上步骤
6. 比较哪次计算中**内群数量**最多，内群最多的那次所建模型就是需要的解

> 注意: 不同问题对应数学模型不同，因此在**计算模型参数时方法必定不同**，RANSAC的作用不在于计算模型参数(导致ransac缺点是模型已知)

要求:

1. 一开始要随机选择的点的数量(n)
2. 需要重复的次数(k)

> 模型必须已知

### 3.4 确定参数(k,n)

1. 假设每个点的真正内群概率 $w$: $w = 内群数目/(内群数目+外群数目)$
2. 参数 $w$ 未知
    1. $w^n$ 表示选择的n个点都是内群的机率
    2. $1-w^n$ 所选择的n个点中至少有一个不是内群的概率
    3. $1-p = (1-w^n)^k$ 重复k次都没有全部n为内群的机率
    4. $p = 1 - (1-w^n)^k$ 算法跑了k次后的成功机率
3. 反求 $k$: $k = log(1-P)/log(1-w^n)$
4. $n$ 不变，$k$ 越大，$p$ 越大; $w$ 不变，$n$ 越大，$k$ 越大
5. 通常 $w$ 未知, $n$ 选小一些好

### 3.5 优缺点

优点:

1. 鲁棒的估计模型参数，从大量局外点的数据集中估计出高精度的参数

缺点:

1. 就按参数迭代次数没上限，上限得到的不一定是最优解
2. RANSAC只有一定的概率得到可信模型，**迭代次数和概率正比**
3. 要求设置和问题相关的阈值
4. RANSAC只能从特定数据集中**估计一个模型**，存在两个或多个模型，RANSAC不能找到别的模型
5. 要求数学模型已知

随机的好处找到最优比遍历快

深度学习: 大数据量的检测，传统方法噪声敏感，速度慢

## 四、哈希算法

### 4.1 Hash

又叫做散列函数(Hash Function)

定义:

1. 从任何一种数据中创建小的数字"指纹"的方法。使得数据量变小, 格式固定
2. 通过哈希一段明文得到哈希值, 哈希值唯一
3. 哈希算法是一个函数, 能把几乎所有的数字文件都转换成一串由数字和字幕构成的看似乱码的字符串

特点:

1. 不可逆性。从输出获得输入很难
2. 输出值唯一性和不可预测性。只要输入信息有一点区别, 哈希输出值也不同。(**可以用来做图像比较**)

分类:

1. 均值哈希算法
2. 差值哈希算法
3. 感知哈希算法

作用:

1. 图像降重
2. 版权维护

### 4.2 汉明距离

两个整数之间汉明距离为两个数字对应**二进制位不同的位置**数目

1: ( 0 0 0 1 )

4: ( 0 1 0 0 )

汉明距离为: 2

### 4.3 均值哈希算法

步骤:

1. 缩放: 图片-> 8*8，保留结构，除去细节 (下采样)
2. 灰度化: 转为灰度图 (如果考虑颜色信息则不做灰度化)
3. 求均值: 计算灰度图所有像素的平均值
4. 比较: 像素值大于**平均值**为1，相反为0，总共64位
5. 生成hash: 将上述步骤生成1和0按顺序组合起来为图片的指纹(hash)
6. 对比指纹: 计算汉明距离，不同位数越少越相似(一般\<5或10被认为相似)

> 注意: 可能下采样的**哈希相同**, 但是**原图不同**(概率相对小), 8*8不同, 哈希也可能一样(因为分析均值)

### 4.4 差值哈希算法

和均值哈希算法前后期基本相同，只有中间比较变化

步骤:

1. 缩放: 图片-> **8\*9**，保留结构，除去细节(下采样)
2. 灰度化: 转为灰度图 (如果考虑颜色信息则不做灰度化)
3. 比较: **像素和后一个比**，大记1，小记0。不和下一行对比，每行9个，8个差值(下采样成8*9原因)
4. 生成hash: 将上述步骤生成1和0按顺序组合起来为图片的指纹(hash)
5. 对比指纹: 计算汉明距离，不同位数越少越相似

### 4.5 算法比较

1. aHash: 均值哈希。速度快，有时不精确。
2. pHash: 感知哈希。精度高，速度较差。
3. dHash: 差值哈希。精度高，且速度较快。(**使用较多**，考虑速度)

缺点: 无法从哈希值返回到缩率图，返回到明文，不可逆性

## 五、拓展-DCT

略
