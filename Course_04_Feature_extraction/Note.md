# 特征选择&特征提取

## 一、特征选择

### 1.1 特征

特征可以分为三类:

1. 相关特征, 对于学习任务有帮助, 可以提升学习算法的效果
2. 无关特征, 对算法没有任何帮助, 不会给算法的效果带来任何提升
3. 冗余特征, 不会对算法带来新的信息

对于特定的学习算法来说, 哪一个特征是有效的是位置的, 因此需要从所有特征中选出有益的特征

特征选择的目的:

1. 降维
2. 降低学习任务难度
3. 提升模型的效率

### 1.2 定义

从N个特征中选择其中M个子特征, 准则函数达到最优解(M个特征是最小集合)

特征选择想要做的是: **选择尽可能少的子特征，模型的效果不会显著下降，并且将结果的类别分布尽可能接近真实的类别分布**

### 1.3 流程

1. 生成过程: 生成候选的特征子集(好坏未知)
2. 评价函数: 评价特征子集的好坏
3. 停止条件: 决定什么时候停止
4. 验证过程: 特征子集是否有效

#### 生成过程

1. 完全搜索: 根据评价函数做完全搜索 (容易达到标准, 耗时耗力)
2. 启发式搜索: 根据一些启发式规则在每次迭代时, 决定剩下特征是应该被选择还是拒绝, 简单且很快 (准确率降低)
3. 随机搜索: 每次迭代设置参数，参数影响特征选择效果(可能得不到最优解)

#### 停止条件

1. 达到最大迭代次数(效率最高)
2. 达到预定最大特征数
3. 增删特征不会产生更好特征子集
4. 根据评价函数，产生最优特征子集

#### 评价函数

类别: filter, wrapper, 组合, embedding

常见评价函数: 距离度量, 信息度量, 依赖度量, 一致性度量, 误分类率度量

## 二、特征提取

通过属性间的关系，组合不同的属性得到新的属性，这样就改变了原来的特征空间

1. 传统: 基于图像本身特征进行提取

2. 深度学习方法: 基于样本自动训练出区分图像的特征分类器

特征选择和特征提取都属于**降维**

## 三、主成分分析PCA

**主要流程:**

1. 对原始数据零均值化(中心化)

2. 求协方差矩阵

3. 对协方差矩阵求特征向量和特征值, 特征向量组成特征空间

降维后的维度不相同, (x, y, z) 中的 (x, y) 不同于 (a, b)

### 3.1 零均值化

平移，使得均值为0(变量减去均值)

计算得到的方向更好的概括原来的数据

原始的回归线可能无法解释数据的分布方向(图)

### 3.2 PCA降维的几何意义

降维的目的: 找一个数据点分布**方差**最大的超平面，让数据在新的坐标轴上足够分散

**主要目的:**

1. 降维后同一维度方差最大

2. 不同维度的相关性为0(排除冗余特征)

### 3.3 协方差

同一元素的协方差就是该元素的方差，不同元素之间的协方差就表示他们的相关性

1. Cov(X,Y) = Cov(Y, X)

2. Cov(aX, bY) = abCov(X, Y)

3. Cov(X1+X2, Y) = Cov(X1, Y) + Cov(X2, Y)

Cov(X, X) = D(X)

Cov(X, Y) = 0 代表X与Y不相关

Cov(X, Y) > 0 代表X与Y正相关

Cov(X, Y) \\< 0 代表X与Y负相关

### 3.4 协方差矩阵

协方差组成协方差矩阵(每个位置为同一样本不同特征的组合)

不同维度的协方差，来源于同一样本

样本矩阵:  每行是一个样本，每列是一个维度，按列计算均值(由x_i1到x_in得到Xi之后进行协方差计算)

协方差矩阵的对角线就是各个维度上的方差

中心化矩阵的协方差矩阵公式: $D=\frac{1}{m}Z^TZ$ (m样本个数)

### 3.5 对协方差矩阵求特征值、特征矩阵

$|A-\lambda E|=0$ 为协方差矩阵A的**特征多项式**

> 求解特征值的过程就是求解特征方程的解

特征向量 = 协方差矩阵的特征

### 3.6 对特征值排序

1. 特征值排序(由大到小, 越大越能反应原始数据)

2. 特征向量按序排列成矩阵$W_{nxk}$

3. $X_{new}\times W$ 投影到特征向量, 得到降维数据集

$X_{new}$待降维的原始矩阵

### 3.7 评价模型好坏, k值确定

计算保留信息量占总信息量百分比

$\eta_k=\frac{\sum^k_{j=1}\lambda_j}{\sum^n_{j=1}\lambda_j} \times 100\%$

计算特征值的和

人为设定阈值

### 3.7 PCA优缺点

**优点:**

1. 无参数限制, 只和数据有关, 和用户无关

2. 降维的同时保留原有数据信息

3. 各主成分之间正交, 可消除相互影响

4. 计算方法简单, 易于实现

**缺点:**

1. 人为无法干预优化

2. 贡献小的主成分往往含有样本差异的重要信息
