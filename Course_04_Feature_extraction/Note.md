# 特征选择&特征提取

## 一、特征选择

### 1.1 特征

特征可以分为三类:

1. 相关特征, 对于学习任务有帮助, 可以提升学习算法的效果
2. 无关特征, 对算法没有任何帮助, 不会给算法的效果带来任何提升
3. 冗余特征, 不会对算法带来新的信息(可以由其他特征推导到)

对于特定的学习算法来说, 哪一个特征是有效的是位置的, 因此需要从所有特征中选出有益的特征

特征选择的目的:

1. 降维
2. 降低学习任务难度
3. 提升模型的效率

### 1.2 定义

从N个特征中选择其中M个子特征, 准则函数达到最优解(M个特征是最小集合)

特征选择想要做的是: **选择尽可能少的子特征，模型的效果不会显著下降，并且将结果的类别分布尽可能接近真实的类别分布**

### 1.3 流程

特征生成的主要四个过程:

1. 生成过程: 生成候选的特征子集(好坏未知)
2. 评价函数: 评价特征子集的好坏
3. 停止条件: 决定什么时候停止
4. 验证过程: 特征子集是否有效

![feature_selection](https://www.researchgate.net/profile/Sofiane-Maza/publication/329467494/figure/fig2/AS:710590704136200@1546429434386/Feature-Selection-Process.png)

#### 1) 生成过程

1. 完全搜索: 根据评价函数做完全搜索 (容易达到标准, 耗时耗力)
2. 启发式搜索: 根据一些启发式规则在每次迭代时, 决定剩下特征是应该被选择还是拒绝, 简单且很快 (准确率降低)
3. 随机搜索: 每次迭代设置参数，参数影响特征选择效果(可能得不到最优解)

#### 2) 停止条件

1. 达到最大迭代次数(效率最高)
2. 达到预定最大特征数
3. 增删特征不会产生更好特征子集
4. 根据评价函数，产生最优特征子集

#### 3) 评价函数

类别: filter, wrapper, 组合, embedding

常见评价函数: 距离度量, 信息度量, 依赖度量, 一致性度量, 误分类率度量

## 二、特征提取

通过属性间的关系，组合不同的属性得到新的属性，这样就改变了原来的特征空间

**区别**: 特征选择是选择子集，特征提取是创造特征

1. 传统: 基于图像本身特征进行提取

2. 深度学习方法: 基于样本自动训练出区分图像的特征分类器

特征选择和特征提取都属于**降维**

## 三、主成分分析(PCA)

**主要流程:**

1. 对原始数据零均值化(中心化)
2. 求协方差矩阵
3. 对协方差矩阵求特征向量和特征值, 特征向量组成特征空间

降维后的维度不相同, (x, y, z) 中的 (x, y) 不同于 (a, b)

![PCA_workflow](https://devopedia.org/images/article/139/4543.1548137789.jpg)

### 3.1 零均值化

步骤: 平移，使得均值为0(变量减去均值)

目的: 计算得到的方向更好的概括原来的数据

原因: 原始的回归线可能无法解释数据的分布方向

![why_centralized](https://datascienceplus.com/wp-content/uploads/2019/09/Capture2.png)

### 3.2 PCA降维的几何意义

降维的目的: 找一个数据点分布**方差**最大的超平面，让数据在新的坐标轴上足够分散

> 如何判断分散：方差大小，方差为0意味着没区别

方差公式:
$$
\begin{equation}
s^2 = \frac{\sum^n_{i=1}(X_i-\bar{X})^2}{n-1}
\end{equation}
$$

**主要目的-核心两步:**

1. 降维后同一维度方差最大
2. 不同维度的相关性为0 (排除冗余特征)

### 3.3 协方差

原理: 同一元素的协方差就是该元素的方差，不同元素之间的协方差就表示他们的相关性

$$
\begin{equation}
cov(X,Y)=\frac{\sum^n_{i=1}(X_i-\bar{X})(Y_i-\bar{Y})}{n-1}
\end{equation}
$$

性质:

1. Cov(X,Y) = Cov(Y, X)
2. Cov(aX, bY) = abCov(X, Y)
3. Cov(X1+X2, Y) = Cov(X1, Y) + Cov(X2, Y)

解释:

1. Cov(X, X) = D(X)
2. Cov(X, Y) = 0 代表X与Y不相关
3. Cov(X, Y) > 0 代表X与Y正相关
4. Cov(X, Y) \\< 0 代表X与Y负相关

目的: 不同维度协方差为0, 同一维度协方差最大

### 3.4 协方差矩阵

协方差组成协方差矩阵(每个位置为同一样本不同特征的组合)

协方差矩阵:
$$
\begin{bmatrix}
    cov(x,x) & cov(x,y) & cov(x,z) \\
    cov(y,x) & cov(y,y) & cov(y,z) \\
    cov(z,x) & cov(z,y) & cov(z,z)
\end{bmatrix}
$$

特点:

1. 不同维度的协方差，来源于同一样本
2. 样本矩阵:  每行是一个样本，每列是一个维度，按列计算均值(由 $x_{i1}$ 到 $x_{in}$ 得到 $X_i$ 之后进行协方差计算)
3. 协方差矩阵的对角线就是各个维度上的方差

中心化矩阵的协方差矩阵公式(m样本个数):

$$
\begin{equation}
D=\frac{1}{m}Z^TZ
\end{equation}
$$

### 3.5 对协方差矩阵求特征值、特征矩阵

$|A-\lambda E|=0$ 为协方差矩阵A的**特征多项式**

> 求解特征值的过程就是求解特征方程的解

特征向量 = 协方差矩阵的特征

同时满足同一维度大方差和特征间相关性为0

> **补充**(关于特征向量): </br>特征向量是可以用来反应矩阵本身固有特征，矩阵作为变换的本质是将对象从一个基底转移到另一个基底，而使用矩阵对特征向量进行线性变换，依然不变。即，**特征值和特征向量的组合能够完全表示一个矩阵**。

### 3.6 对特征值排序

排序流程:

1. 特征值排序(由大到小, 越大越能反应原始数据)
2. 特征向量按序排列，选择最大的k个特征向量组成矩阵 $W_{nxk}$
3. $X_{new}\times W$ 投影到特征向量, 得到降维数据集，其中 $X_{new}$ 是待降维的原始矩阵。

### 3.7 评价模型好坏, k值确定

计算保留信息量占总信息量百分比，判断特征值的和是否符合人为设定阈值

$$
\begin{equation}
\eta_k=\frac{\sum^k_{j=1}\lambda_j}{\sum^n_{j=1}\lambda_j} \times 100\%
\end{equation}
$$

### 3.7 PCA优缺点

**优点:**

1. 无参数限制, 只和数据有关, 和用户无关
2. 降维的同时保留原有数据信息
3. 各主成分之间正交, 可消除相互影响
4. 计算方法简单, 易于实现

**缺点:**

1. 人为无法干预优化
2. 贡献小的主成分往往含有样本差异的重要信息
